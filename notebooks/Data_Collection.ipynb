{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a86fa213-5c40-4668-8575-6f33313db139",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa194dc-e039-46af-b43b-a85fcc02f4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4cbb3c-7c30-4059-9934-3c5b75010b4a",
   "metadata": {},
   "source": [
    "## Step 2: Define Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c13ff6-8570-4755-82a5-90f67fbff09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# NYC CitiBike Demand Analysis 2022\n",
       "\n",
       "## Project Overview\n",
       "- **Data Source**: Citi Bike System Data\n",
       "- **Analysis Period**: 2022\n",
       "- **Weather Station**: LaGuardia Airport (NYC)\n",
       "- **Station ID**: GHCND:USW00014732\n",
       "\n",
       "## Analysis Objectives\n",
       "1. Identify peak demand periods and popular stations\n",
       "2. Analyze seasonal patterns and weather impact\n",
       "3. Optimize bike distribution across NYC\n",
       "4. Identify expansion opportunities\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA Token: Loaded\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Use correct path for notebooks directory\n",
    "\n",
    "DATA_PATH = \"../data/raw/\"\n",
    "OUTPUT_PATH = \"../data/processed/\"\n",
    "NOAA_TOKEN = os.getenv('NOAA_TOKEN')\n",
    "\n",
    "# Display project information\n",
    "project_info = {\n",
    "    \"project_name\": \"NYC CitiBike Demand Analysis 2022\",\n",
    "    \"data_source\": \"Citi Bike System Data\",\n",
    "    \"year\": 2022,\n",
    "    \"weather_station\": \"LaGuardia Airport (NYC)\",\n",
    "    \"station_id\": \"GHCND:USW00014732\"\n",
    "}\n",
    "\n",
    "markdown_content = f\"\"\"\n",
    "# {project_info['project_name']}\n",
    "\n",
    "## Project Overview\n",
    "- **Data Source**: {project_info['data_source']}\n",
    "- **Analysis Period**: {project_info['year']}\n",
    "- **Weather Station**: {project_info['weather_station']}\n",
    "- **Station ID**: {project_info['station_id']}\n",
    "\n",
    "## Analysis Objectives\n",
    "1. Identify peak demand periods and popular stations\n",
    "2. Analyze seasonal patterns and weather impact\n",
    "3. Optimize bike distribution across NYC\n",
    "4. Identify expansion opportunities\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_content))\n",
    "print(f\"NOAA Token: {'Loaded' if NOAA_TOKEN else 'NOT FOUND - Check .env file'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e39d5-9da3-48ca-8dd2-3efe483b3f08",
   "metadata": {},
   "source": [
    "## Step 3: Load CtiBike Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6aa27-65ff-4e52-9147-585cdd4ddd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CitiBike data files...\n",
      "Using data path: ../data/raw/\n",
      "Path exists: True\n",
      "Found 38 CSV files to process\n",
      "First 5 files:\n",
      "  1. 202201-citibike-tripdata_1.csv (185.56 MB)\n",
      "  2. 202201-citibike-tripdata_2.csv (4.5 MB)\n",
      "  3. 202202-citibike-tripdata_1.csv (185.96 MB)\n",
      "  4. 202202-citibike-tripdata_2.csv (36.84 MB)\n",
      "  5. 202203-citibike-tripdata_1.csv (186.27 MB)\n",
      "\n",
      "Starting to load files...\n",
      "Reading: 202201-citibike-tripdata_1.csv (185.6 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202201-citibike-tripdata_2.csv (4.5 MB)\n",
      "  Success: 24,555 rows, 14 columns\n",
      "Reading: 202202-citibike-tripdata_1.csv (186.0 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202202-citibike-tripdata_2.csv (36.8 MB)\n",
      "  Success: 197,312 rows, 14 columns\n",
      "Reading: 202203-citibike-tripdata_1.csv (186.3 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202203-citibike-tripdata_2.csv (157.5 MB)\n",
      "  Success: 845,965 rows, 14 columns\n",
      "Reading: 202204-citibike-tripdata_1.csv (186.4 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202204-citibike-tripdata_2.csv (186.5 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202204-citibike-tripdata_3.csv (48.7 MB)\n",
      "  Success: 260,790 rows, 14 columns\n",
      "Reading: 202205-citibike-tripdata_1.csv (186.8 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202205-citibike-tripdata_2.csv (186.8 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202205-citibike-tripdata_3.csv (161.0 MB)\n",
      "  Success: 865,425 rows, 14 columns\n",
      "Reading: 202206-citibike-tripdata_1.csv (186.8 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202206-citibike-tripdata_2.csv (186.7 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202206-citibike-tripdata_3.csv (186.8 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202206-citibike-tripdata_4.csv (64.4 MB)\n",
      "  Success: 343,914 rows, 14 columns\n",
      "Reading: 202207-citibike-tripdata_1.csv (186.6 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202207-citibike-tripdata_2.csv (186.5 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202207-citibike-tripdata_3.csv (187.6 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202207-citibike-tripdata_4.csv (74.2 MB)\n",
      "  Success: 397,932 rows, 14 columns\n",
      "Reading: 202208-citibike-tripdata_1.csv (187.7 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202208-citibike-tripdata_2.csv (187.0 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202208-citibike-tripdata_3.csv (185.8 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202208-citibike-tripdata_4.csv (107.9 MB)\n",
      "  Success: 576,020 rows, 14 columns\n",
      "Reading: 202209-citibike-tripdata_1.csv (186.4 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202209-citibike-tripdata_2.csv (186.8 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202209-citibike-tripdata_3.csv (187.4 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202209-citibike-tripdata_4.csv (76.8 MB)\n",
      "  Success: 411,866 rows, 14 columns\n",
      "Reading: 202210-citibike-tripdata_1.csv (186.3 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202210-citibike-tripdata_2.csv (187.2 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202210-citibike-tripdata_3.csv (175.5 MB)\n",
      "  Success: 936,584 rows, 14 columns\n",
      "Reading: 202211-citibike-tripdata_1.csv (186.8 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202211-citibike-tripdata_2.csv (186.6 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202211-citibike-tripdata_3.csv (72.2 MB)\n",
      "  Success: 386,394 rows, 14 columns\n",
      "Reading: 202212-citibike-tripdata_1.csv (186.4 MB)\n",
      "  Success: 1,000,000 rows, 14 columns\n",
      "Reading: 202212-citibike-tripdata_2.csv (111.0 MB)\n",
      "  Success: 592,049 rows, 14 columns\n",
      "Reading: nyc_citibike_2022_processed.csv (7122.3 MB)\n"
     ]
    }
   ],
   "source": [
    "# Load CitiBike Data\n",
    "\n",
    "print(\"Loading CitiBike data files...\")\n",
    "\n",
    "print(f\"Using data path: {DATA_PATH}\")\n",
    "print(f\"Path exists: {os.path.exists(DATA_PATH)}\")\n",
    "\n",
    "# Get all CSV files\n",
    "all_files = []\n",
    "for item in os.listdir(DATA_PATH):\n",
    "    if item.endswith('.csv') and '2022' in item and 'citibike' in item:\n",
    "        full_path = os.path.join(DATA_PATH, item)\n",
    "        all_files.append(full_path)\n",
    "\n",
    "all_files = sorted(all_files)\n",
    "file_count = len(all_files)\n",
    "\n",
    "print(f\"Found {file_count} CSV files to process\")\n",
    "\n",
    "# Display first few files\n",
    "print(\"First 5 files:\")\n",
    "for i, file_path in enumerate(all_files[:5], 1):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_size = round(os.path.getsize(file_path) / (1024*1024), 2)\n",
    "    print(f\"  {i}. {file_name} ({file_size} MB)\")\n",
    "\n",
    "def read_citibike_file(file_path):\n",
    "    try:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
    "        print(f\"Reading: {file_name} ({file_size:.1f} MB)\")\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        df['_source_file'] = file_name\n",
    "        print(f\"  Success: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"\\nStarting to load files...\")\n",
    "dataframes = []\n",
    "for file_path in all_files:\n",
    "    df = read_citibike_file(file_path)\n",
    "    if df is not None:\n",
    "        dataframes.append(df)\n",
    "\n",
    "if dataframes:\n",
    "    df_bikes = pd.concat(dataframes, ignore_index=True)\n",
    "    print(f\"\\n=== SUCCESS ===\")\n",
    "    print(f\"Successfully concatenated {len(dataframes)} files\")\n",
    "    print(f\"Total dataset: {len(df_bikes):,} rows, {len(df_bikes.columns)} columns\")\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(f\"\\nDate range:\")\n",
    "    # Find datetime columns\n",
    "    datetime_cols = [col for col in df_bikes.columns if 'time' in col.lower() or 'date' in col.lower()]\n",
    "    if datetime_cols:\n",
    "        for col in datetime_cols[:2]:  # Check first 2 datetime columns\n",
    "            if col in df_bikes.columns:\n",
    "                print(f\"  {col}: {df_bikes[col].min()} to {df_bikes[col].max()}\")\n",
    "    \n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    display(df_bikes.head(5))\n",
    "    \n",
    "else:\n",
    "    raise Exception(\"No data files were successfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd4a16-17a6-430f-91db-9c33503cb205",
   "metadata": {},
   "source": [
    "## Step 4: Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0347dcf-3488-47bd-97c2-09c5c1acc750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning & Preprocessing\n",
    "\n",
    "print(\"Starting data cleaning and preprocessing...\")\n",
    "\n",
    "# Convert datetime columns\n",
    "print(\"Converting datetime columns...\")\n",
    "df_bikes['started_at'] = pd.to_datetime(df_bikes['started_at'], errors='coerce')\n",
    "df_bikes['ended_at'] = pd.to_datetime(df_bikes['ended_at'], errors='coerce')\n",
    "\n",
    "# Extract date for weather merging\n",
    "df_bikes['date'] = df_bikes['started_at'].dt.date\n",
    "df_bikes['date'] = pd.to_datetime(df_bikes['date'])\n",
    "\n",
    "# Data quality check\n",
    "initial_rows = len(df_bikes)\n",
    "df_bikes = df_bikes.dropna(subset=['started_at', 'ended_at'])\n",
    "final_rows = len(df_bikes)\n",
    "removed_rows = initial_rows - final_rows\n",
    "\n",
    "print(f\"Data cleaning complete:\")\n",
    "print(f\"  - Initial rows: {initial_rows:,}\")\n",
    "print(f\"  - Rows removed (invalid dates): {removed_rows:,}\")\n",
    "print(f\"  - Final rows: {final_rows:,}\")\n",
    "print(f\"  - Data retention: {round((final_rows/initial_rows)*100, 2)}%\")\n",
    "\n",
    "# Display date range\n",
    "print(f\"\\nDate range:\")\n",
    "print(f\"  - Earliest trip: {df_bikes['started_at'].min()}\")\n",
    "print(f\"  - Latest trip: {df_bikes['started_at'].max()}\")\n",
    "print(f\"  - Total days: {df_bikes['date'].nunique()}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(f\"  - Member vs Casual:\")\n",
    "print(f\"    {df_bikes['member_casual'].value_counts().to_dict()}\")\n",
    "print(f\"  - Bike types:\")\n",
    "print(f\"    {df_bikes['rideable_type'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef37bfb3-9402-443a-b3f2-c934a5d41b20",
   "metadata": {},
   "source": [
    "## Step 5: Fetch Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c3cf46-ee06-49e0-9036-633f1cb13146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Weather Data from NOAA API\n",
    "\n",
    "print(\"Fetching weather data from NOAA API...\")\n",
    "\n",
    "station_id = \"GHCND:USW00014732\"  # LaGuardia Airport, NYC\n",
    "start_date = \"2022-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "noaa_url = (\n",
    "    f\"https://www.ncdc.noaa.gov/cdo-web/api/v2/data?\"\n",
    "    f\"datasetid=GHCND&datatypeid=TAVG&limit=1000&\"\n",
    "    f\"stationid={station_id}&startdate={start_date}&enddate={end_date}\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    headers = {'token': NOAA_TOKEN}\n",
    "    response = requests.get(noaa_url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        weather_data = json.loads(response.text)\n",
    "        \n",
    "        if 'results' in weather_data and weather_data['results']:\n",
    "            # Extract temperature data\n",
    "            avg_temps = [item for item in weather_data['results'] if item['datatype'] == 'TAVG']\n",
    "            dates_temp = [item['date'] for item in avg_temps]\n",
    "            temps = [item['value'] for item in avg_temps]\n",
    "            \n",
    "            # Create weather dataframe\n",
    "            df_weather = pd.DataFrame()\n",
    "            df_weather['date'] = [datetime.strptime(d, \"%Y-%m-%dT%H:%M:%S\") for d in dates_temp]\n",
    "            df_weather['avg_temp_c'] = [float(v) / 10.0 for v in temps]  # Convert to Celsius\n",
    "            df_weather['avg_temp_f'] = df_weather['avg_temp_c'] * 9/5 + 32  # Convert to Fahrenheit\n",
    "            \n",
    "            print(f\"Successfully retrieved {len(df_weather)} days of weather data\")\n",
    "            \n",
    "            # Display weather summary\n",
    "            print(f\"\\nWeather data summary:\")\n",
    "            print(f\"  - Date range: {df_weather['date'].min().strftime('%Y-%m-%d')} to {df_weather['date'].max().strftime('%Y-%m-%d')}\")\n",
    "            print(f\"  - Average temperature: {df_weather['avg_temp_c'].mean():.1f}째C ({df_weather['avg_temp_f'].mean():.1f}째F)\")\n",
    "            print(f\"  - Min temperature: {df_weather['avg_temp_c'].min():.1f}째C\")\n",
    "            print(f\"  - Max temperature: {df_weather['avg_temp_c'].max():.1f}째C\")\n",
    "            \n",
    "        else:\n",
    "            print(\"No weather data returned from API\")\n",
    "            df_weather = pd.DataFrame(columns=['date', 'avg_temp_c', 'avg_temp_f'])\n",
    "            \n",
    "    else:\n",
    "        print(f\"API request failed with status code: {response.status_code}\")\n",
    "        df_weather = pd.DataFrame(columns=['date', 'avg_temp_c', 'avg_temp_f'])\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error fetching weather data: {e}\")\n",
    "    df_weather = pd.DataFrame(columns=['date', 'avg_temp_c', 'avg_temp_f'])\n",
    "\n",
    "# Display sample of weather data\n",
    "if not df_weather.empty:\n",
    "    print(f\"\\nSample of weather data:\")\n",
    "    display(df_weather.head())\n",
    "else:\n",
    "    print(\"No weather data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344d06a-b72c-4f1f-9287-6bfadf4350b0",
   "metadata": {},
   "source": [
    "## Step 6: Merge Datastes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06715e7-e1b8-44ec-ab9f-478c892613bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Bike Data with Weather Data\n",
    "\n",
    "print(\"Merging bike data with weather data...\")\n",
    "\n",
    "if not df_weather.empty:\n",
    "    df_merged = df_bikes.merge(df_weather, how='left', on='date', indicator=True)\n",
    "    \n",
    "    # Analyze merge results\n",
    "    merge_stats = df_merged['_merge'].value_counts()\n",
    "    \n",
    "    print(f\"Merge completed:\")\n",
    "    print(f\"  - Both (successful matches): {merge_stats.get('both', 0):,} rows\")\n",
    "    print(f\"  - Left only (no weather data): {merge_stats.get('left_only', 0):,} rows\")\n",
    "    print(f\"  - Right only (extra weather data): {merge_stats.get('right_only', 0):,} rows\")\n",
    "    print(f\"  - Merge success rate: {round((merge_stats.get('both', 0) / len(df_merged)) * 100, 2)}%\")\n",
    "    \n",
    "    # Remove merge indicator column\n",
    "    df_merged = df_merged.drop('_merge', axis=1)\n",
    "    \n",
    "else:\n",
    "    df_merged = df_bikes.copy()\n",
    "    df_merged['avg_temp_c'] = np.nan\n",
    "    df_merged['avg_temp_f'] = np.nan\n",
    "    print(\"No weather data available - proceeding with NaN values for temperature\")\n",
    "\n",
    "print(f\"\\nFinal merged dataset:\")\n",
    "print(f\"  - Total rows: {len(df_merged):,}\")\n",
    "print(f\"  - Total columns: {len(df_merged.columns)}\")\n",
    "print(f\"  - Memory usage: {round(df_merged.memory_usage(deep=True).sum() / (1024**3), 2)} GB\")\n",
    "\n",
    "# Display final dataset structure\n",
    "print(f\"\\nFinal dataset columns:\")\n",
    "for col in df_merged.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c495ef-3f90-4012-8738-7b08af0afff9",
   "metadata": {},
   "source": [
    "## Step 7: Export Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9147b460-8fb4-44a1-9672-ffe80749cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Processed Data\n",
    "\n",
    "print(\"Exporting processed data...\")\n",
    "\n",
    "output_file = os.path.join(OUTPUT_PATH, \"nyc_citibike_2022_processed.csv\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "\n",
    "# Save to CSV\n",
    "df_merged.to_csv(output_file, index=False)\n",
    "\n",
    "# Verify file was created\n",
    "if os.path.exists(output_file):\n",
    "    file_size = round(os.path.getsize(output_file) / (1024**3), 2)\n",
    "    print(f\"Export completed successfully:\")\n",
    "    print(f\"  - File size: {file_size} GB\")\n",
    "    print(f\"  - Total rows: {len(df_merged):,}\")\n",
    "    print(f\"  - Total columns: {len(df_merged.columns)}\")\n",
    "    print(f\"  - Export timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Display sample of exported data\n",
    "    print(f\"\\nSample of exported data:\")\n",
    "    sample_cols = ['started_at', 'ended_at', 'start_station_name', 'end_station_name', 'member_casual', 'avg_temp_c']\n",
    "    available_cols = [col for col in sample_cols if col in df_merged.columns]\n",
    "    display(df_merged[available_cols].head(5))\n",
    "    \n",
    "else:\n",
    "    print(\"Error: Output file was not created successfully\")\n",
    "    raise Exception(\"Data export failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f63608-09d2-4490-a2d8-6343b2544126",
   "metadata": {},
   "source": [
    "## Step 8: Project Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934701ea-4252-494d-8e43-a765cc848483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Summary\n",
    "print(\"Generating project summary...\")\n",
    "\n",
    "summary_stats = {\n",
    "    'total_trips': len(df_merged),\n",
    "    'date_range_days': df_merged['date'].nunique(),\n",
    "    'data_files_processed': len(dataframes),\n",
    "    'weather_days_integrated': len(df_weather) if not df_weather.empty else 0,\n",
    "    'memory_usage_gb': round(df_merged.memory_usage(deep=True).sum() / (1024**3), 2),\n",
    "    'completion_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# Calculate additional statistics\n",
    "user_types = df_merged['member_casual'].value_counts()\n",
    "bike_types = df_merged['rideable_type'].value_counts()\n",
    "\n",
    "markdown_content = f\"\"\"\n",
    "# Data Collection Phase Complete\n",
    "\n",
    "## Project Summary\n",
    "\n",
    "### Data Processing Results\n",
    "- **Total Bike Trips Processed**: {summary_stats['total_trips']:,}\n",
    "- **Data Files Consolidated**: {summary_stats['data_files_processed']}\n",
    "- **Days in Dataset**: {summary_stats['date_range_days']}\n",
    "- **Weather Data Points**: {summary_stats['weather_days_integrated']}\n",
    "- **Final Dataset Size**: {summary_stats['memory_usage_gb']} GB\n",
    "\n",
    "### User Statistics\n",
    "- **Member Rides**: {user_types.get('member', 0):,}\n",
    "- **Casual Rides**: {user_types.get('casual', 0):,}\n",
    "\n",
    "### Bike Type Distribution\n",
    "\"\"\"\n",
    "\n",
    "for bike_type, count in bike_types.items():\n",
    "    markdown_content += f\"- **{bike_type}**: {count:,}\\n\"\n",
    "\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503a053-506f-4e80-9a6c-470f1ae51be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
